{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jupyter nbconvert --execute --to html MicrosoftPrediction.ipynb --ExecutePreprocessor.timeout=-1\n",
    "\n",
    "Above is the command needed to convert this to an html file. Set the timeout to -1 so the converter doesn't end the process early.\n",
    "\n",
    "The goal of this analysis is to familiarize myself with time-series analysis and to run a sample simulation at the end to see how the model's accuracy would play out in the real world, assuming I buy as sell a single stock at a time, buying at the opening price for a day and selling at the closing price.\n",
    "\n",
    "Note: I read a previous approach piror to my analysis at\n",
    "\n",
    "https://towardsdatascience.com/predicting-stock-price-with-lstm-13af86a74944\n",
    "\n",
    "by Asutosh Nayak\n",
    "\n",
    "I improved upon the ideas here to suit my own understanding of the problem and to simplify a lot of preprocessing required for the LSTM input. The architecture used for the LSTM was my own choice, as well as running a simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yahoo_fin import stock_info as si\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the last 2 months of data to predict new day value so 'TIME_STEPS' = 60 days. Since the predictions are a day in advance 'DAYS_TO_PREDICT' = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = 60\n",
    "DAYS_TO_PREDICT = 1\n",
    "\n",
    "# Use scalers to normalize values between 0 and 1\n",
    "dataScaler = MinMaxScaler(feature_range=(0, 1))\n",
    "outputScaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "def buildTimeSeries(start_date, end_date):\n",
    "    msft = si.get_data('MSFT', start_date=start_date, end_date=end_date)\n",
    "    \n",
    "    # Normalize values for training\n",
    "    msftScaled = dataScaler.fit_transform(\n",
    "        msft[['open', 'close', 'high', 'low', 'adjclose', 'volume']].values)\n",
    "    outputScaled = outputScaler.fit_transform(msft[['close']].values)\n",
    "    \n",
    "    # Keep track of each day's input (data) and output (labels) values to train and test predictive model\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(TIME_STEPS, len(msft)-DAYS_TO_PREDICT):\n",
    "        \n",
    "        # Get dataframe of info from 60 days prior\n",
    "        oldPrices = msftScaled[i-TIME_STEPS:i, :]\n",
    "        data.append(oldPrices)\n",
    "        # Get closing price for day after the oldPrices\n",
    "        desiredOutput = outputScaled[i: i + DAYS_TO_PREDICT]\n",
    "        labels.append(desiredOutput)\n",
    "        \n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    labels = np.squeeze(labels)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on 13 years worth of data and test on 2018-2019 prices\n",
    "train_start='01/01/2005'\n",
    "train_end='12/31/2017'\n",
    "xTrain, yTrain = buildTimeSeries(train_start, train_end)\n",
    "\n",
    "test_start='01/01/2018'\n",
    "test_end='05/31/2019'\n",
    "xTest, yTest = buildTimeSeries(test_start, test_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3211, 60, 6)\n",
      "(3211,)\n",
      "(293, 60, 6)\n",
      "(293,)\n"
     ]
    }
   ],
   "source": [
    "# Ensure the shapes are correct for NN input/output\n",
    "print(xTrain.shape)\n",
    "print(yTrain.shape)\n",
    "\n",
    "print(xTest.shape)\n",
    "print(yTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM models are particularly useful when doing time series analysis because of the RNN architecture which allows a kind of memory to be stored of previous outputs. Although the prediction is only one day in advance, this general architecture should be able to be used for forecasting multiple days in advance (albeit with less accuracy).\n",
    "\n",
    "tensorboard --logdir=./logs\n",
    "\n",
    "The above terminal command can be used for tensorflow visualization at http://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 58, 32)            608       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 14, 32)            8320      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 17,281\n",
      "Trainable params: 17,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2568 samples, validate on 643 samples\n",
      "Epoch 1/100\n",
      "2568/2568 [==============================] - ETA: 3:01 - loss: 0.045 - ETA: 1:00 - loss: 0.040 - ETA: 29s - loss: 0.043 - ETA: 17s - loss: 0.03 - ETA: 11s - loss: 0.03 - ETA: 8s - loss: 0.0321 - ETA: 6s - loss: 0.029 - ETA: 5s - loss: 0.027 - ETA: 4s - loss: 0.025 - ETA: 3s - loss: 0.023 - ETA: 3s - loss: 0.021 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.011 - 4s 2ms/step - loss: 0.0111 - val_loss: 0.0172\n",
      "Epoch 2/100\n",
      "2568/2568 [==============================] - ETA: 2s - loss: 7.6731e-0 - ETA: 1s - loss: 7.3888e-0 - ETA: 1s - loss: 8.6894e-0 - ETA: 1s - loss: 8.0645e-0 - ETA: 1s - loss: 8.1071e-0 - ETA: 1s - loss: 8.2992e-0 - ETA: 0s - loss: 8.1965e-0 - ETA: 0s - loss: 8.0838e-0 - ETA: 0s - loss: 7.8927e-0 - ETA: 0s - loss: 7.9081e-0 - ETA: 0s - loss: 7.8020e-0 - ETA: 0s - loss: 7.8086e-0 - ETA: 0s - loss: 7.6167e-0 - ETA: 0s - loss: 7.5089e-0 - ETA: 0s - loss: 7.4960e-0 - ETA: 0s - loss: 7.4022e-0 - ETA: 0s - loss: 7.4526e-0 - ETA: 0s - loss: 7.4911e-0 - ETA: 0s - loss: 7.4047e-0 - ETA: 0s - loss: 7.4526e-0 - ETA: 0s - loss: 7.3649e-0 - 1s 499us/step - loss: 7.3546e-04 - val_loss: 0.0083\n",
      "Epoch 3/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 5.0166e-0 - ETA: 0s - loss: 7.3264e-0 - ETA: 0s - loss: 6.8067e-0 - ETA: 0s - loss: 6.6604e-0 - ETA: 0s - loss: 6.6054e-0 - ETA: 0s - loss: 6.8135e-0 - ETA: 0s - loss: 6.8307e-0 - ETA: 0s - loss: 6.7348e-0 - ETA: 0s - loss: 6.6860e-0 - ETA: 0s - loss: 6.6440e-0 - ETA: 0s - loss: 6.5828e-0 - ETA: 0s - loss: 6.5332e-0 - ETA: 0s - loss: 6.5658e-0 - ETA: 0s - loss: 6.4969e-0 - ETA: 0s - loss: 6.5641e-0 - ETA: 0s - loss: 6.5842e-0 - ETA: 0s - loss: 6.4725e-0 - ETA: 0s - loss: 6.5156e-0 - ETA: 0s - loss: 6.4228e-0 - ETA: 0s - loss: 6.3773e-0 - 1s 452us/step - loss: 6.3601e-04 - val_loss: 0.0038\n",
      "Epoch 4/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 5.5466e-0 - ETA: 1s - loss: 6.0045e-0 - ETA: 0s - loss: 5.6809e-0 - ETA: 0s - loss: 5.8493e-0 - ETA: 0s - loss: 5.9233e-0 - ETA: 0s - loss: 5.7984e-0 - ETA: 0s - loss: 5.6290e-0 - ETA: 0s - loss: 5.6588e-0 - ETA: 0s - loss: 5.6885e-0 - ETA: 0s - loss: 5.9375e-0 - ETA: 0s - loss: 5.8968e-0 - ETA: 0s - loss: 5.8320e-0 - ETA: 0s - loss: 5.8637e-0 - ETA: 0s - loss: 5.8184e-0 - ETA: 0s - loss: 5.8886e-0 - ETA: 0s - loss: 5.8384e-0 - ETA: 0s - loss: 5.8709e-0 - ETA: 0s - loss: 5.8351e-0 - ETA: 0s - loss: 5.7931e-0 - ETA: 0s - loss: 5.8306e-0 - 1s 440us/step - loss: 5.8220e-04 - val_loss: 0.0070\n",
      "Epoch 5/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 4.6492e-0 - ETA: 0s - loss: 7.4198e-0 - ETA: 0s - loss: 6.2513e-0 - ETA: 0s - loss: 6.2229e-0 - ETA: 0s - loss: 6.0092e-0 - ETA: 0s - loss: 5.7654e-0 - ETA: 0s - loss: 5.7525e-0 - ETA: 0s - loss: 5.5029e-0 - ETA: 0s - loss: 5.4467e-0 - ETA: 0s - loss: 5.4391e-0 - ETA: 0s - loss: 5.3815e-0 - ETA: 0s - loss: 5.4652e-0 - ETA: 0s - loss: 5.3907e-0 - ETA: 0s - loss: 5.3712e-0 - ETA: 0s - loss: 5.3597e-0 - ETA: 0s - loss: 5.3735e-0 - ETA: 0s - loss: 5.3954e-0 - ETA: 0s - loss: 5.3196e-0 - ETA: 0s - loss: 5.3454e-0 - ETA: 0s - loss: 5.3857e-0 - 1s 471us/step - loss: 5.3452e-04 - val_loss: 0.0039\n",
      "Epoch 6/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 5.6301e-0 - ETA: 1s - loss: 6.3008e-0 - ETA: 1s - loss: 5.6443e-0 - ETA: 0s - loss: 5.0114e-0 - ETA: 0s - loss: 5.3582e-0 - ETA: 0s - loss: 5.2794e-0 - ETA: 0s - loss: 5.3006e-0 - ETA: 0s - loss: 5.1158e-0 - ETA: 0s - loss: 5.1030e-0 - ETA: 0s - loss: 4.9482e-0 - ETA: 0s - loss: 4.9537e-0 - ETA: 0s - loss: 5.0332e-0 - ETA: 0s - loss: 5.0135e-0 - ETA: 0s - loss: 5.0398e-0 - ETA: 0s - loss: 4.9705e-0 - ETA: 0s - loss: 4.9244e-0 - ETA: 0s - loss: 4.9567e-0 - ETA: 0s - loss: 5.0560e-0 - ETA: 0s - loss: 5.0191e-0 - ETA: 0s - loss: 4.9743e-0 - 1s 459us/step - loss: 5.0230e-04 - val_loss: 0.0036\n",
      "Epoch 7/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 2.2612e-0 - ETA: 1s - loss: 5.1213e-0 - ETA: 0s - loss: 4.8824e-0 - ETA: 0s - loss: 4.7227e-0 - ETA: 0s - loss: 5.2570e-0 - ETA: 0s - loss: 5.1033e-0 - ETA: 0s - loss: 5.0710e-0 - ETA: 0s - loss: 4.8816e-0 - ETA: 0s - loss: 4.7634e-0 - ETA: 0s - loss: 4.6622e-0 - ETA: 0s - loss: 4.6701e-0 - ETA: 0s - loss: 4.6501e-0 - ETA: 0s - loss: 4.6182e-0 - ETA: 0s - loss: 4.6253e-0 - ETA: 0s - loss: 4.5327e-0 - ETA: 0s - loss: 4.5683e-0 - ETA: 0s - loss: 4.5843e-0 - ETA: 0s - loss: 4.6282e-0 - ETA: 0s - loss: 4.6793e-0 - ETA: 0s - loss: 4.6888e-0 - ETA: 0s - loss: 4.6593e-0 - 1s 505us/step - loss: 4.6385e-04 - val_loss: 0.0052\n",
      "Epoch 8/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 5.2041e-0 - ETA: 1s - loss: 4.1711e-0 - ETA: 1s - loss: 4.4935e-0 - ETA: 1s - loss: 4.7999e-0 - ETA: 1s - loss: 4.6893e-0 - ETA: 0s - loss: 4.4126e-0 - ETA: 0s - loss: 4.2930e-0 - ETA: 0s - loss: 4.2398e-0 - ETA: 0s - loss: 4.2693e-0 - ETA: 0s - loss: 4.2756e-0 - ETA: 0s - loss: 4.2337e-0 - ETA: 0s - loss: 4.2770e-0 - ETA: 0s - loss: 4.2055e-0 - ETA: 0s - loss: 4.3022e-0 - ETA: 0s - loss: 4.2476e-0 - ETA: 0s - loss: 4.2516e-0 - ETA: 0s - loss: 4.3989e-0 - ETA: 0s - loss: 4.4519e-0 - ETA: 0s - loss: 4.4456e-0 - ETA: 0s - loss: 4.4066e-0 - ETA: 0s - loss: 4.3935e-0 - ETA: 0s - loss: 4.3561e-0 - ETA: 0s - loss: 4.3929e-0 - 1s 563us/step - loss: 4.3638e-04 - val_loss: 0.0015\n",
      "Epoch 9/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 3.6992e-0 - ETA: 1s - loss: 4.3805e-0 - ETA: 1s - loss: 4.5616e-0 - ETA: 1s - loss: 4.2702e-0 - ETA: 1s - loss: 4.6555e-0 - ETA: 0s - loss: 4.6685e-0 - ETA: 0s - loss: 4.6316e-0 - ETA: 0s - loss: 4.4501e-0 - ETA: 0s - loss: 4.4402e-0 - ETA: 0s - loss: 4.4031e-0 - ETA: 0s - loss: 4.3685e-0 - ETA: 0s - loss: 4.3860e-0 - ETA: 0s - loss: 4.3222e-0 - ETA: 0s - loss: 4.3448e-0 - ETA: 0s - loss: 4.2556e-0 - ETA: 0s - loss: 4.2275e-0 - ETA: 0s - loss: 4.2213e-0 - ETA: 0s - loss: 4.1760e-0 - ETA: 0s - loss: 4.1592e-0 - ETA: 0s - loss: 4.1214e-0 - ETA: 0s - loss: 4.0963e-0 - ETA: 0s - loss: 4.0922e-0 - ETA: 0s - loss: 4.0810e-0 - 1s 557us/step - loss: 4.0767e-04 - val_loss: 0.0011\n",
      "Epoch 10/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 5.8849e-0 - ETA: 1s - loss: 3.8929e-0 - ETA: 1s - loss: 3.5237e-0 - ETA: 1s - loss: 4.1034e-0 - ETA: 1s - loss: 4.0615e-0 - ETA: 1s - loss: 4.0628e-0 - ETA: 0s - loss: 3.9290e-0 - ETA: 0s - loss: 3.8944e-0 - ETA: 0s - loss: 3.8729e-0 - ETA: 0s - loss: 3.8396e-0 - ETA: 0s - loss: 3.9360e-0 - ETA: 0s - loss: 3.9070e-0 - ETA: 0s - loss: 4.0873e-0 - ETA: 0s - loss: 4.0470e-0 - ETA: 0s - loss: 3.9770e-0 - ETA: 0s - loss: 3.9485e-0 - ETA: 0s - loss: 3.8885e-0 - ETA: 0s - loss: 3.9004e-0 - ETA: 0s - loss: 3.8600e-0 - ETA: 0s - loss: 3.8403e-0 - ETA: 0s - loss: 3.9467e-0 - ETA: 0s - loss: 3.9756e-0 - ETA: 0s - loss: 3.9277e-0 - 1s 549us/step - loss: 3.9020e-04 - val_loss: 0.0012\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2568/2568 [==============================] - ETA: 1s - loss: 3.2231e-0 - ETA: 1s - loss: 4.4036e-0 - ETA: 1s - loss: 4.0758e-0 - ETA: 1s - loss: 4.4360e-0 - ETA: 1s - loss: 4.0632e-0 - ETA: 1s - loss: 4.3011e-0 - ETA: 0s - loss: 4.1922e-0 - ETA: 0s - loss: 4.1967e-0 - ETA: 0s - loss: 4.2387e-0 - ETA: 0s - loss: 4.1663e-0 - ETA: 0s - loss: 4.1619e-0 - ETA: 0s - loss: 4.1025e-0 - ETA: 0s - loss: 3.9378e-0 - ETA: 0s - loss: 3.8556e-0 - ETA: 0s - loss: 3.8655e-0 - ETA: 0s - loss: 3.8007e-0 - ETA: 0s - loss: 3.7921e-0 - ETA: 0s - loss: 3.7467e-0 - ETA: 0s - loss: 3.7963e-0 - ETA: 0s - loss: 3.7951e-0 - ETA: 0s - loss: 3.8183e-0 - ETA: 0s - loss: 3.8643e-0 - ETA: 0s - loss: 3.8296e-0 - ETA: 0s - loss: 3.8044e-0 - ETA: 0s - loss: 3.8570e-0 - ETA: 0s - loss: 3.8772e-0 - 2s 619us/step - loss: 3.8690e-04 - val_loss: 0.0014\n",
      "Epoch 12/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 2.6630e-0 - ETA: 1s - loss: 3.3360e-0 - ETA: 1s - loss: 3.3974e-0 - ETA: 1s - loss: 3.1896e-0 - ETA: 1s - loss: 3.0488e-0 - ETA: 0s - loss: 2.8705e-0 - ETA: 0s - loss: 2.8642e-0 - ETA: 0s - loss: 3.1693e-0 - ETA: 0s - loss: 3.2936e-0 - ETA: 0s - loss: 3.5119e-0 - ETA: 0s - loss: 3.4813e-0 - ETA: 0s - loss: 3.4105e-0 - ETA: 0s - loss: 3.3816e-0 - ETA: 0s - loss: 3.4795e-0 - ETA: 0s - loss: 3.4671e-0 - ETA: 0s - loss: 3.4074e-0 - ETA: 0s - loss: 3.4615e-0 - ETA: 0s - loss: 3.6924e-0 - ETA: 0s - loss: 3.7215e-0 - ETA: 0s - loss: 3.6801e-0 - ETA: 0s - loss: 3.6699e-0 - ETA: 0s - loss: 3.6877e-0 - ETA: 0s - loss: 3.6349e-0 - ETA: 0s - loss: 3.6296e-0 - 1s 549us/step - loss: 3.6312e-04 - val_loss: 0.0010\n",
      "Epoch 13/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 1.9058e-0 - ETA: 1s - loss: 3.5142e-0 - ETA: 1s - loss: 3.4175e-0 - ETA: 1s - loss: 3.4224e-0 - ETA: 1s - loss: 3.5180e-0 - ETA: 0s - loss: 3.9159e-0 - ETA: 0s - loss: 4.0475e-0 - ETA: 0s - loss: 3.9630e-0 - ETA: 0s - loss: 3.8394e-0 - ETA: 0s - loss: 3.9066e-0 - ETA: 0s - loss: 3.9047e-0 - ETA: 0s - loss: 3.7644e-0 - ETA: 0s - loss: 3.6268e-0 - ETA: 0s - loss: 3.6170e-0 - ETA: 0s - loss: 3.7275e-0 - ETA: 0s - loss: 3.7190e-0 - ETA: 0s - loss: 3.7184e-0 - ETA: 0s - loss: 3.7151e-0 - ETA: 0s - loss: 3.7399e-0 - ETA: 0s - loss: 3.7294e-0 - ETA: 0s - loss: 3.6557e-0 - 1s 495us/step - loss: 3.6506e-04 - val_loss: 8.8521e-04\n",
      "Epoch 14/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 1.7251e-0 - ETA: 1s - loss: 3.1961e-0 - ETA: 1s - loss: 3.1095e-0 - ETA: 0s - loss: 3.5381e-0 - ETA: 0s - loss: 3.7426e-0 - ETA: 0s - loss: 3.6670e-0 - ETA: 0s - loss: 3.4887e-0 - ETA: 0s - loss: 3.4498e-0 - ETA: 0s - loss: 3.5123e-0 - ETA: 0s - loss: 3.4798e-0 - ETA: 0s - loss: 3.6706e-0 - ETA: 0s - loss: 3.7323e-0 - ETA: 0s - loss: 3.8291e-0 - ETA: 0s - loss: 3.8011e-0 - ETA: 0s - loss: 3.7385e-0 - ETA: 0s - loss: 3.6376e-0 - ETA: 0s - loss: 3.6211e-0 - ETA: 0s - loss: 3.6172e-0 - ETA: 0s - loss: 3.6095e-0 - ETA: 0s - loss: 3.6465e-0 - 1s 509us/step - loss: 3.6195e-04 - val_loss: 8.6093e-04\n",
      "Epoch 15/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 2.9345e-0 - ETA: 1s - loss: 3.5587e-0 - ETA: 1s - loss: 3.3870e-0 - ETA: 1s - loss: 3.3391e-0 - ETA: 0s - loss: 3.7142e-0 - ETA: 0s - loss: 3.6102e-0 - ETA: 0s - loss: 3.4459e-0 - ETA: 0s - loss: 3.4489e-0 - ETA: 0s - loss: 3.5694e-0 - ETA: 0s - loss: 3.5755e-0 - ETA: 0s - loss: 3.5613e-0 - ETA: 0s - loss: 3.6071e-0 - ETA: 0s - loss: 3.6602e-0 - ETA: 0s - loss: 3.7078e-0 - ETA: 0s - loss: 3.6457e-0 - ETA: 0s - loss: 3.6781e-0 - ETA: 0s - loss: 3.6373e-0 - ETA: 0s - loss: 3.6498e-0 - ETA: 0s - loss: 3.6042e-0 - ETA: 0s - loss: 3.6032e-0 - ETA: 0s - loss: 3.5979e-0 - 1s 505us/step - loss: 3.5932e-04 - val_loss: 8.4186e-04\n",
      "Epoch 16/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 2.3010e-0 - ETA: 1s - loss: 2.6580e-0 - ETA: 1s - loss: 3.1570e-0 - ETA: 1s - loss: 3.7679e-0 - ETA: 0s - loss: 3.7707e-0 - ETA: 0s - loss: 3.6318e-0 - ETA: 0s - loss: 3.5625e-0 - ETA: 0s - loss: 3.6471e-0 - ETA: 0s - loss: 3.6703e-0 - ETA: 0s - loss: 3.7682e-0 - ETA: 0s - loss: 3.7651e-0 - ETA: 0s - loss: 3.7316e-0 - ETA: 0s - loss: 3.6885e-0 - ETA: 0s - loss: 3.6408e-0 - ETA: 0s - loss: 3.6772e-0 - ETA: 0s - loss: 3.6285e-0 - ETA: 0s - loss: 3.5597e-0 - ETA: 0s - loss: 3.5638e-0 - ETA: 0s - loss: 3.5544e-0 - ETA: 0s - loss: 3.5199e-0 - 1s 482us/step - loss: 3.5011e-04 - val_loss: 9.4992e-04\n",
      "Epoch 17/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 2.8598e-0 - ETA: 1s - loss: 3.5902e-0 - ETA: 0s - loss: 3.2742e-0 - ETA: 0s - loss: 3.9171e-0 - ETA: 0s - loss: 3.6872e-0 - ETA: 0s - loss: 3.6474e-0 - ETA: 0s - loss: 3.5990e-0 - ETA: 0s - loss: 3.5891e-0 - ETA: 0s - loss: 3.5347e-0 - ETA: 0s - loss: 3.4106e-0 - ETA: 0s - loss: 3.4667e-0 - ETA: 0s - loss: 3.4149e-0 - ETA: 0s - loss: 3.4526e-0 - ETA: 0s - loss: 3.4118e-0 - ETA: 0s - loss: 3.4190e-0 - ETA: 0s - loss: 3.4327e-0 - ETA: 0s - loss: 3.4981e-0 - ETA: 0s - loss: 3.4947e-0 - ETA: 0s - loss: 3.4563e-0 - ETA: 0s - loss: 3.4415e-0 - ETA: 0s - loss: 3.4784e-0 - 1s 484us/step - loss: 3.4765e-04 - val_loss: 0.0011\n",
      "Epoch 18/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 3.5266e-0 - ETA: 0s - loss: 3.7524e-0 - ETA: 0s - loss: 3.3861e-0 - ETA: 0s - loss: 3.2582e-0 - ETA: 0s - loss: 3.2139e-0 - ETA: 0s - loss: 3.4359e-0 - ETA: 0s - loss: 3.5474e-0 - ETA: 0s - loss: 3.3344e-0 - ETA: 0s - loss: 3.3609e-0 - ETA: 0s - loss: 3.2855e-0 - ETA: 0s - loss: 3.2807e-0 - ETA: 0s - loss: 3.2303e-0 - ETA: 0s - loss: 3.2395e-0 - ETA: 0s - loss: 3.3075e-0 - ETA: 0s - loss: 3.3392e-0 - ETA: 0s - loss: 3.4136e-0 - ETA: 0s - loss: 3.4124e-0 - ETA: 0s - loss: 3.3552e-0 - ETA: 0s - loss: 3.3273e-0 - ETA: 0s - loss: 3.3575e-0 - 1s 459us/step - loss: 3.3899e-04 - val_loss: 0.0011\n",
      "Epoch 19/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 3.7569e-0 - ETA: 1s - loss: 3.0575e-0 - ETA: 1s - loss: 3.0409e-0 - ETA: 0s - loss: 3.1717e-0 - ETA: 0s - loss: 3.3686e-0 - ETA: 0s - loss: 3.2820e-0 - ETA: 0s - loss: 3.5050e-0 - ETA: 0s - loss: 3.4165e-0 - ETA: 0s - loss: 3.3147e-0 - ETA: 0s - loss: 3.3328e-0 - ETA: 0s - loss: 3.3207e-0 - ETA: 0s - loss: 3.2400e-0 - ETA: 0s - loss: 3.2912e-0 - ETA: 0s - loss: 3.2653e-0 - ETA: 0s - loss: 3.2683e-0 - ETA: 0s - loss: 3.2442e-0 - ETA: 0s - loss: 3.2386e-0 - ETA: 0s - loss: 3.2070e-0 - ETA: 0s - loss: 3.3120e-0 - 1s 451us/step - loss: 3.3822e-04 - val_loss: 7.9612e-04\n",
      "Epoch 20/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 2.6312e-0 - ETA: 0s - loss: 3.3739e-0 - ETA: 0s - loss: 3.6114e-0 - ETA: 0s - loss: 4.0251e-0 - ETA: 0s - loss: 3.8600e-0 - ETA: 0s - loss: 3.6625e-0 - ETA: 0s - loss: 3.4719e-0 - ETA: 0s - loss: 3.3461e-0 - ETA: 0s - loss: 3.3025e-0 - ETA: 0s - loss: 3.2623e-0 - ETA: 0s - loss: 3.2268e-0 - ETA: 0s - loss: 3.2640e-0 - ETA: 0s - loss: 3.2165e-0 - ETA: 0s - loss: 3.2452e-0 - ETA: 0s - loss: 3.3202e-0 - ETA: 0s - loss: 3.3672e-0 - ETA: 0s - loss: 3.3919e-0 - ETA: 0s - loss: 3.3983e-0 - ETA: 0s - loss: 3.4307e-0 - ETA: 0s - loss: 3.4479e-0 - 1s 465us/step - loss: 3.4149e-04 - val_loss: 9.9940e-04\n",
      "Epoch 21/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 3.5213e-0 - ETA: 0s - loss: 2.7950e-0 - ETA: 0s - loss: 2.6862e-0 - ETA: 0s - loss: 2.9519e-0 - ETA: 0s - loss: 3.0691e-0 - ETA: 0s - loss: 3.1942e-0 - ETA: 0s - loss: 3.1695e-0 - ETA: 0s - loss: 3.1220e-0 - ETA: 0s - loss: 3.1996e-0 - ETA: 0s - loss: 3.2477e-0 - ETA: 0s - loss: 3.2077e-0 - ETA: 0s - loss: 3.1149e-0 - ETA: 0s - loss: 3.1783e-0 - ETA: 0s - loss: 3.1827e-0 - ETA: 0s - loss: 3.2121e-0 - ETA: 0s - loss: 3.2597e-0 - ETA: 0s - loss: 3.2828e-0 - ETA: 0s - loss: 3.2364e-0 - ETA: 0s - loss: 3.2521e-0 - ETA: 0s - loss: 3.2412e-0 - 1s 466us/step - loss: 3.2607e-04 - val_loss: 0.0026\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2568/2568 [==============================] - ETA: 0s - loss: 7.2214e-0 - ETA: 0s - loss: 3.8416e-0 - ETA: 0s - loss: 3.6158e-0 - ETA: 0s - loss: 3.6819e-0 - ETA: 0s - loss: 3.5464e-0 - ETA: 0s - loss: 3.3584e-0 - ETA: 0s - loss: 3.3442e-0 - ETA: 0s - loss: 3.3663e-0 - ETA: 0s - loss: 3.3748e-0 - ETA: 0s - loss: 3.3159e-0 - ETA: 0s - loss: 3.3172e-0 - ETA: 0s - loss: 3.4754e-0 - ETA: 0s - loss: 3.4889e-0 - ETA: 0s - loss: 3.4102e-0 - ETA: 0s - loss: 3.4582e-0 - ETA: 0s - loss: 3.4148e-0 - ETA: 0s - loss: 3.3627e-0 - ETA: 0s - loss: 3.3458e-0 - ETA: 0s - loss: 3.3459e-0 - ETA: 0s - loss: 3.2872e-0 - 1s 471us/step - loss: 3.3880e-04 - val_loss: 8.1138e-04\n",
      "Epoch 23/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 3.5401e-0 - ETA: 1s - loss: 2.8411e-0 - ETA: 1s - loss: 2.7599e-0 - ETA: 1s - loss: 2.7968e-0 - ETA: 0s - loss: 2.6933e-0 - ETA: 0s - loss: 2.6825e-0 - ETA: 0s - loss: 2.6060e-0 - ETA: 0s - loss: 2.6823e-0 - ETA: 0s - loss: 2.7721e-0 - ETA: 0s - loss: 2.8981e-0 - ETA: 0s - loss: 2.8743e-0 - ETA: 0s - loss: 2.9070e-0 - ETA: 0s - loss: 2.9958e-0 - ETA: 0s - loss: 3.0693e-0 - ETA: 0s - loss: 3.1292e-0 - ETA: 0s - loss: 3.1724e-0 - ETA: 0s - loss: 3.1632e-0 - ETA: 0s - loss: 3.1359e-0 - ETA: 0s - loss: 3.2360e-0 - ETA: 0s - loss: 3.1995e-0 - ETA: 0s - loss: 3.2410e-0 - ETA: 0s - loss: 3.2434e-0 - 1s 528us/step - loss: 3.2433e-04 - val_loss: 0.0015\n",
      "Epoch 24/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 3.3342e-0 - ETA: 1s - loss: 4.0024e-0 - ETA: 1s - loss: 3.4754e-0 - ETA: 1s - loss: 3.4755e-0 - ETA: 0s - loss: 3.3150e-0 - ETA: 0s - loss: 3.1831e-0 - ETA: 0s - loss: 3.2188e-0 - ETA: 0s - loss: 3.2600e-0 - ETA: 0s - loss: 3.1054e-0 - ETA: 0s - loss: 3.1811e-0 - ETA: 0s - loss: 3.0943e-0 - ETA: 0s - loss: 3.0600e-0 - ETA: 0s - loss: 3.0366e-0 - ETA: 0s - loss: 2.9802e-0 - ETA: 0s - loss: 3.0246e-0 - ETA: 0s - loss: 2.9601e-0 - ETA: 0s - loss: 3.0463e-0 - ETA: 0s - loss: 3.0311e-0 - ETA: 0s - loss: 3.1604e-0 - ETA: 0s - loss: 3.2102e-0 - ETA: 0s - loss: 3.2595e-0 - 1s 522us/step - loss: 3.2716e-04 - val_loss: 0.0012\n",
      "Epoch 25/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 3.2621e-0 - ETA: 1s - loss: 3.8190e-0 - ETA: 1s - loss: 2.9262e-0 - ETA: 1s - loss: 3.6614e-0 - ETA: 0s - loss: 3.5108e-0 - ETA: 0s - loss: 3.2143e-0 - ETA: 0s - loss: 3.3125e-0 - ETA: 0s - loss: 3.2669e-0 - ETA: 0s - loss: 3.4984e-0 - ETA: 0s - loss: 3.4420e-0 - ETA: 0s - loss: 3.4561e-0 - ETA: 0s - loss: 3.4585e-0 - ETA: 0s - loss: 3.4164e-0 - ETA: 0s - loss: 3.4313e-0 - ETA: 0s - loss: 3.4586e-0 - ETA: 0s - loss: 3.3811e-0 - ETA: 0s - loss: 3.4064e-0 - ETA: 0s - loss: 3.3372e-0 - ETA: 0s - loss: 3.3135e-0 - ETA: 0s - loss: 3.2669e-0 - ETA: 0s - loss: 3.2456e-0 - ETA: 0s - loss: 3.2129e-0 - ETA: 0s - loss: 3.2208e-0 - 1s 556us/step - loss: 3.2229e-04 - val_loss: 0.0012\n",
      "Epoch 26/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 3.3639e-0 - ETA: 1s - loss: 3.3589e-0 - ETA: 1s - loss: 3.4073e-0 - ETA: 1s - loss: 3.1784e-0 - ETA: 1s - loss: 3.1973e-0 - ETA: 1s - loss: 3.1132e-0 - ETA: 1s - loss: 3.0270e-0 - ETA: 1s - loss: 3.1447e-0 - ETA: 0s - loss: 3.2206e-0 - ETA: 0s - loss: 3.2649e-0 - ETA: 0s - loss: 3.1738e-0 - ETA: 0s - loss: 3.2195e-0 - ETA: 0s - loss: 3.1433e-0 - ETA: 0s - loss: 3.0738e-0 - ETA: 0s - loss: 3.0878e-0 - ETA: 0s - loss: 3.1008e-0 - ETA: 0s - loss: 3.1095e-0 - ETA: 0s - loss: 3.0690e-0 - ETA: 0s - loss: 3.1711e-0 - ETA: 0s - loss: 3.1668e-0 - ETA: 0s - loss: 3.1908e-0 - ETA: 0s - loss: 3.1959e-0 - ETA: 0s - loss: 3.1637e-0 - ETA: 0s - loss: 3.2002e-0 - ETA: 0s - loss: 3.2626e-0 - ETA: 0s - loss: 3.2643e-0 - 2s 599us/step - loss: 3.2492e-04 - val_loss: 0.0010\n",
      "Epoch 27/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 3.2346e-0 - ETA: 1s - loss: 3.2844e-0 - ETA: 1s - loss: 3.0035e-0 - ETA: 1s - loss: 2.8287e-0 - ETA: 1s - loss: 3.1430e-0 - ETA: 0s - loss: 3.0492e-0 - ETA: 0s - loss: 3.1516e-0 - ETA: 0s - loss: 3.1669e-0 - ETA: 0s - loss: 3.3083e-0 - ETA: 0s - loss: 3.2310e-0 - ETA: 0s - loss: 3.3130e-0 - ETA: 0s - loss: 3.2458e-0 - ETA: 0s - loss: 3.3407e-0 - ETA: 0s - loss: 3.3133e-0 - ETA: 0s - loss: 3.2424e-0 - ETA: 0s - loss: 3.2243e-0 - ETA: 0s - loss: 3.2007e-0 - ETA: 0s - loss: 3.1879e-0 - ETA: 0s - loss: 3.1663e-0 - ETA: 0s - loss: 3.2099e-0 - ETA: 0s - loss: 3.1963e-0 - ETA: 0s - loss: 3.2585e-0 - ETA: 0s - loss: 3.2304e-0 - ETA: 0s - loss: 3.2399e-0 - ETA: 0s - loss: 3.2249e-0 - 1s 557us/step - loss: 3.2090e-04 - val_loss: 7.2732e-04\n",
      "Epoch 28/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 2.5972e-0 - ETA: 1s - loss: 2.1964e-0 - ETA: 1s - loss: 2.4772e-0 - ETA: 1s - loss: 3.1978e-0 - ETA: 1s - loss: 3.2631e-0 - ETA: 1s - loss: 3.1924e-0 - ETA: 1s - loss: 3.2522e-0 - ETA: 1s - loss: 3.3530e-0 - ETA: 0s - loss: 3.3743e-0 - ETA: 0s - loss: 3.4931e-0 - ETA: 0s - loss: 3.4399e-0 - ETA: 0s - loss: 3.5437e-0 - ETA: 0s - loss: 3.4148e-0 - ETA: 0s - loss: 3.4158e-0 - ETA: 0s - loss: 3.3798e-0 - ETA: 0s - loss: 3.2885e-0 - ETA: 0s - loss: 3.3146e-0 - ETA: 0s - loss: 3.2863e-0 - ETA: 0s - loss: 3.2387e-0 - ETA: 0s - loss: 3.2497e-0 - ETA: 0s - loss: 3.2291e-0 - ETA: 0s - loss: 3.1756e-0 - ETA: 0s - loss: 3.1688e-0 - ETA: 0s - loss: 3.1815e-0 - ETA: 0s - loss: 3.2338e-0 - ETA: 0s - loss: 3.1948e-0 - ETA: 0s - loss: 3.1531e-0 - 2s 623us/step - loss: 3.1510e-04 - val_loss: 7.6092e-04\n",
      "Epoch 29/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 2.3812e-0 - ETA: 1s - loss: 2.4497e-0 - ETA: 1s - loss: 2.4992e-0 - ETA: 1s - loss: 2.7539e-0 - ETA: 1s - loss: 2.9451e-0 - ETA: 1s - loss: 3.0622e-0 - ETA: 1s - loss: 3.4215e-0 - ETA: 1s - loss: 3.3200e-0 - ETA: 1s - loss: 3.2253e-0 - ETA: 0s - loss: 3.2553e-0 - ETA: 0s - loss: 3.2559e-0 - ETA: 0s - loss: 3.2504e-0 - ETA: 0s - loss: 3.1412e-0 - ETA: 0s - loss: 3.2921e-0 - ETA: 0s - loss: 3.3565e-0 - ETA: 0s - loss: 3.4103e-0 - ETA: 0s - loss: 3.3310e-0 - ETA: 0s - loss: 3.3444e-0 - ETA: 0s - loss: 3.3148e-0 - ETA: 0s - loss: 3.2391e-0 - ETA: 0s - loss: 3.2179e-0 - ETA: 0s - loss: 3.2346e-0 - ETA: 0s - loss: 3.1879e-0 - 1s 533us/step - loss: 3.1823e-04 - val_loss: 7.3322e-04\n",
      "Epoch 30/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 1.3267e-0 - ETA: 1s - loss: 2.5815e-0 - ETA: 0s - loss: 2.9531e-0 - ETA: 0s - loss: 2.7583e-0 - ETA: 0s - loss: 2.9399e-0 - ETA: 0s - loss: 2.9376e-0 - ETA: 0s - loss: 2.9708e-0 - ETA: 0s - loss: 2.9455e-0 - ETA: 0s - loss: 2.9058e-0 - ETA: 0s - loss: 3.0128e-0 - ETA: 0s - loss: 3.0185e-0 - ETA: 0s - loss: 3.0895e-0 - ETA: 0s - loss: 3.1884e-0 - ETA: 0s - loss: 3.1414e-0 - ETA: 0s - loss: 3.1815e-0 - ETA: 0s - loss: 3.1463e-0 - ETA: 0s - loss: 3.1191e-0 - ETA: 0s - loss: 3.1548e-0 - ETA: 0s - loss: 3.1316e-0 - ETA: 0s - loss: 3.1021e-0 - 1s 456us/step - loss: 3.1059e-04 - val_loss: 0.0012\n",
      "Epoch 31/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 4.0164e-0 - ETA: 0s - loss: 4.3122e-0 - ETA: 0s - loss: 3.5962e-0 - ETA: 0s - loss: 3.2932e-0 - ETA: 0s - loss: 3.3133e-0 - ETA: 0s - loss: 3.1915e-0 - ETA: 0s - loss: 2.9948e-0 - ETA: 0s - loss: 3.0805e-0 - ETA: 0s - loss: 3.1586e-0 - ETA: 0s - loss: 3.1422e-0 - ETA: 0s - loss: 3.0544e-0 - ETA: 0s - loss: 3.0275e-0 - ETA: 0s - loss: 3.0228e-0 - ETA: 0s - loss: 3.0286e-0 - ETA: 0s - loss: 3.0878e-0 - ETA: 0s - loss: 3.0717e-0 - ETA: 0s - loss: 3.1151e-0 - ETA: 0s - loss: 3.1074e-0 - ETA: 0s - loss: 3.0676e-0 - ETA: 0s - loss: 3.0695e-0 - 1s 477us/step - loss: 3.0390e-04 - val_loss: 8.6561e-04\n",
      "Epoch 32/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 1.5511e-0 - ETA: 1s - loss: 2.7103e-0 - ETA: 1s - loss: 3.1224e-0 - ETA: 0s - loss: 3.3823e-0 - ETA: 0s - loss: 3.1497e-0 - ETA: 0s - loss: 3.2316e-0 - ETA: 0s - loss: 3.1649e-0 - ETA: 0s - loss: 3.1510e-0 - ETA: 0s - loss: 3.0298e-0 - ETA: 0s - loss: 3.0022e-0 - ETA: 0s - loss: 2.9884e-0 - ETA: 0s - loss: 2.9601e-0 - ETA: 0s - loss: 2.9628e-0 - ETA: 0s - loss: 3.0506e-0 - ETA: 0s - loss: 3.0016e-0 - ETA: 0s - loss: 2.9937e-0 - ETA: 0s - loss: 3.0864e-0 - ETA: 0s - loss: 3.0969e-0 - ETA: 0s - loss: 3.0762e-0 - ETA: 0s - loss: 3.0488e-0 - 1s 457us/step - loss: 3.0311e-04 - val_loss: 8.9311e-04\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2568/2568 [==============================] - ETA: 1s - loss: 2.8263e-0 - ETA: 1s - loss: 2.6520e-0 - ETA: 0s - loss: 2.7773e-0 - ETA: 0s - loss: 2.8181e-0 - ETA: 0s - loss: 2.8892e-0 - ETA: 0s - loss: 2.9434e-0 - ETA: 0s - loss: 3.1054e-0 - ETA: 0s - loss: 2.9916e-0 - ETA: 0s - loss: 2.9065e-0 - ETA: 0s - loss: 2.8638e-0 - ETA: 0s - loss: 2.8720e-0 - ETA: 0s - loss: 2.9119e-0 - ETA: 0s - loss: 2.9272e-0 - ETA: 0s - loss: 2.8724e-0 - ETA: 0s - loss: 2.8270e-0 - ETA: 0s - loss: 2.7875e-0 - ETA: 0s - loss: 2.8618e-0 - ETA: 0s - loss: 2.8643e-0 - ETA: 0s - loss: 2.9028e-0 - ETA: 0s - loss: 2.9566e-0 - ETA: 0s - loss: 2.9858e-0 - 1s 524us/step - loss: 2.9762e-04 - val_loss: 0.0010\n",
      "Epoch 34/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 2.8441e-0 - ETA: 1s - loss: 2.5780e-0 - ETA: 1s - loss: 2.7542e-0 - ETA: 0s - loss: 3.3738e-0 - ETA: 0s - loss: 3.2081e-0 - ETA: 0s - loss: 3.0840e-0 - ETA: 0s - loss: 2.9808e-0 - ETA: 0s - loss: 3.0202e-0 - ETA: 0s - loss: 2.9610e-0 - ETA: 0s - loss: 3.0062e-0 - ETA: 0s - loss: 2.9110e-0 - ETA: 0s - loss: 2.9321e-0 - ETA: 0s - loss: 2.9303e-0 - ETA: 0s - loss: 2.8682e-0 - ETA: 0s - loss: 2.8460e-0 - ETA: 0s - loss: 2.8015e-0 - ETA: 0s - loss: 2.7893e-0 - ETA: 0s - loss: 2.7791e-0 - ETA: 0s - loss: 2.8671e-0 - ETA: 0s - loss: 2.9044e-0 - ETA: 0s - loss: 2.9670e-0 - ETA: 0s - loss: 2.9885e-0 - ETA: 0s - loss: 2.9741e-0 - 1s 542us/step - loss: 2.9713e-04 - val_loss: 7.0094e-04\n",
      "Epoch 35/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 1.3408e-0 - ETA: 1s - loss: 3.0276e-0 - ETA: 1s - loss: 2.8173e-0 - ETA: 1s - loss: 2.7717e-0 - ETA: 0s - loss: 2.6803e-0 - ETA: 0s - loss: 2.8000e-0 - ETA: 0s - loss: 2.8444e-0 - ETA: 0s - loss: 2.9035e-0 - ETA: 0s - loss: 2.7746e-0 - ETA: 0s - loss: 2.7171e-0 - ETA: 0s - loss: 2.8627e-0 - ETA: 0s - loss: 2.8691e-0 - ETA: 0s - loss: 2.9176e-0 - ETA: 0s - loss: 2.8875e-0 - ETA: 0s - loss: 2.8668e-0 - ETA: 0s - loss: 2.8039e-0 - ETA: 0s - loss: 2.8517e-0 - ETA: 0s - loss: 2.9784e-0 - ETA: 0s - loss: 2.9599e-0 - ETA: 0s - loss: 2.9492e-0 - 1s 460us/step - loss: 2.9563e-04 - val_loss: 0.0015\n",
      "Epoch 36/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 2.0488e-0 - ETA: 0s - loss: 2.9504e-0 - ETA: 0s - loss: 3.0112e-0 - ETA: 0s - loss: 2.9658e-0 - ETA: 0s - loss: 3.1439e-0 - ETA: 0s - loss: 3.1806e-0 - ETA: 0s - loss: 3.1716e-0 - ETA: 0s - loss: 3.1636e-0 - ETA: 0s - loss: 3.2387e-0 - ETA: 0s - loss: 3.1765e-0 - ETA: 0s - loss: 3.1532e-0 - ETA: 0s - loss: 3.1041e-0 - ETA: 0s - loss: 3.1283e-0 - ETA: 0s - loss: 3.0671e-0 - ETA: 0s - loss: 3.0008e-0 - ETA: 0s - loss: 2.9826e-0 - ETA: 0s - loss: 2.9196e-0 - ETA: 0s - loss: 2.9188e-0 - ETA: 0s - loss: 2.8923e-0 - ETA: 0s - loss: 2.8834e-0 - ETA: 0s - loss: 2.9442e-0 - 1s 501us/step - loss: 2.9315e-04 - val_loss: 7.3303e-04\n",
      "Epoch 37/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 1.2249e-0 - ETA: 1s - loss: 2.3132e-0 - ETA: 1s - loss: 2.5994e-0 - ETA: 0s - loss: 2.5587e-0 - ETA: 0s - loss: 2.6948e-0 - ETA: 0s - loss: 2.8538e-0 - ETA: 0s - loss: 2.8623e-0 - ETA: 0s - loss: 2.9196e-0 - ETA: 0s - loss: 2.8714e-0 - ETA: 0s - loss: 2.9033e-0 - ETA: 0s - loss: 2.8674e-0 - ETA: 0s - loss: 2.8053e-0 - ETA: 0s - loss: 2.8202e-0 - ETA: 0s - loss: 2.8033e-0 - ETA: 0s - loss: 2.9250e-0 - ETA: 0s - loss: 2.9385e-0 - ETA: 0s - loss: 3.0101e-0 - ETA: 0s - loss: 3.0655e-0 - ETA: 0s - loss: 2.9858e-0 - ETA: 0s - loss: 2.9787e-0 - ETA: 0s - loss: 2.9583e-0 - 1s 520us/step - loss: 2.9509e-04 - val_loss: 9.7773e-04\n",
      "Epoch 38/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 2.9675e-0 - ETA: 1s - loss: 2.9245e-0 - ETA: 1s - loss: 3.1247e-0 - ETA: 1s - loss: 3.3355e-0 - ETA: 0s - loss: 3.1475e-0 - ETA: 0s - loss: 3.0134e-0 - ETA: 0s - loss: 2.8227e-0 - ETA: 0s - loss: 3.2112e-0 - ETA: 0s - loss: 3.0825e-0 - ETA: 0s - loss: 3.0297e-0 - ETA: 0s - loss: 3.1226e-0 - ETA: 0s - loss: 3.0571e-0 - ETA: 0s - loss: 3.0872e-0 - ETA: 0s - loss: 3.0409e-0 - ETA: 0s - loss: 3.0138e-0 - ETA: 0s - loss: 2.9903e-0 - ETA: 0s - loss: 3.0062e-0 - ETA: 0s - loss: 2.9305e-0 - ETA: 0s - loss: 2.9038e-0 - ETA: 0s - loss: 2.9539e-0 - ETA: 0s - loss: 2.9107e-0 - 1s 513us/step - loss: 2.8867e-04 - val_loss: 9.9776e-04\n",
      "Epoch 39/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 3.7066e-0 - ETA: 1s - loss: 2.4818e-0 - ETA: 1s - loss: 2.5464e-0 - ETA: 1s - loss: 2.5014e-0 - ETA: 0s - loss: 2.6773e-0 - ETA: 0s - loss: 2.7484e-0 - ETA: 0s - loss: 2.6752e-0 - ETA: 0s - loss: 2.7975e-0 - ETA: 0s - loss: 2.8658e-0 - ETA: 0s - loss: 2.8577e-0 - ETA: 0s - loss: 2.9325e-0 - ETA: 0s - loss: 2.9190e-0 - ETA: 0s - loss: 2.9114e-0 - ETA: 0s - loss: 2.8629e-0 - ETA: 0s - loss: 2.9124e-0 - ETA: 0s - loss: 2.9221e-0 - ETA: 0s - loss: 2.9280e-0 - ETA: 0s - loss: 2.8760e-0 - ETA: 0s - loss: 2.8500e-0 - ETA: 0s - loss: 2.8843e-0 - ETA: 0s - loss: 2.8432e-0 - 1s 515us/step - loss: 2.8389e-04 - val_loss: 0.0014\n",
      "Epoch 40/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 3.1788e-0 - ETA: 1s - loss: 2.5837e-0 - ETA: 1s - loss: 2.8522e-0 - ETA: 1s - loss: 3.0932e-0 - ETA: 0s - loss: 3.1919e-0 - ETA: 0s - loss: 3.1254e-0 - ETA: 0s - loss: 3.0005e-0 - ETA: 0s - loss: 2.8530e-0 - ETA: 0s - loss: 2.8618e-0 - ETA: 0s - loss: 2.7828e-0 - ETA: 0s - loss: 2.9002e-0 - ETA: 0s - loss: 2.8720e-0 - ETA: 0s - loss: 2.8129e-0 - ETA: 0s - loss: 2.8302e-0 - ETA: 0s - loss: 2.8567e-0 - ETA: 0s - loss: 2.8221e-0 - ETA: 0s - loss: 2.7825e-0 - ETA: 0s - loss: 2.7549e-0 - ETA: 0s - loss: 2.7773e-0 - ETA: 0s - loss: 2.7606e-0 - 1s 491us/step - loss: 2.7558e-04 - val_loss: 7.2576e-04\n",
      "Epoch 41/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 3.8486e-0 - ETA: 1s - loss: 3.2859e-0 - ETA: 1s - loss: 3.2346e-0 - ETA: 0s - loss: 3.0277e-0 - ETA: 0s - loss: 3.3924e-0 - ETA: 0s - loss: 3.2146e-0 - ETA: 0s - loss: 3.0213e-0 - ETA: 0s - loss: 3.0783e-0 - ETA: 0s - loss: 2.9878e-0 - ETA: 0s - loss: 2.9696e-0 - ETA: 0s - loss: 3.0234e-0 - ETA: 0s - loss: 3.0059e-0 - ETA: 0s - loss: 3.0617e-0 - ETA: 0s - loss: 3.0469e-0 - ETA: 0s - loss: 2.9532e-0 - ETA: 0s - loss: 2.9348e-0 - ETA: 0s - loss: 2.9550e-0 - ETA: 0s - loss: 2.9116e-0 - ETA: 0s - loss: 2.8716e-0 - ETA: 0s - loss: 2.8388e-0 - ETA: 0s - loss: 2.8110e-0 - ETA: 0s - loss: 2.8130e-0 - ETA: 0s - loss: 2.8002e-0 - 1s 555us/step - loss: 2.7981e-04 - val_loss: 0.0015\n",
      "Epoch 42/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 2.3837e-0 - ETA: 1s - loss: 2.1952e-0 - ETA: 1s - loss: 2.3871e-0 - ETA: 1s - loss: 2.6774e-0 - ETA: 1s - loss: 2.8627e-0 - ETA: 0s - loss: 3.1198e-0 - ETA: 0s - loss: 3.1711e-0 - ETA: 0s - loss: 3.1041e-0 - ETA: 0s - loss: 2.9838e-0 - ETA: 0s - loss: 3.0262e-0 - ETA: 0s - loss: 2.9832e-0 - ETA: 0s - loss: 2.8646e-0 - ETA: 0s - loss: 2.8899e-0 - ETA: 0s - loss: 2.8780e-0 - ETA: 0s - loss: 2.9026e-0 - ETA: 0s - loss: 2.8770e-0 - ETA: 0s - loss: 2.8582e-0 - ETA: 0s - loss: 2.7985e-0 - ETA: 0s - loss: 2.8034e-0 - ETA: 0s - loss: 2.7828e-0 - ETA: 0s - loss: 2.7944e-0 - ETA: 0s - loss: 2.7992e-0 - ETA: 0s - loss: 2.8166e-0 - ETA: 0s - loss: 2.8092e-0 - 1s 577us/step - loss: 2.8021e-04 - val_loss: 0.0011\n",
      "Epoch 43/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 2.8024e-0 - ETA: 1s - loss: 2.2805e-0 - ETA: 1s - loss: 3.2039e-0 - ETA: 1s - loss: 2.8002e-0 - ETA: 1s - loss: 2.6759e-0 - ETA: 1s - loss: 2.7256e-0 - ETA: 1s - loss: 2.7013e-0 - ETA: 1s - loss: 2.9592e-0 - ETA: 1s - loss: 3.0303e-0 - ETA: 0s - loss: 2.9715e-0 - ETA: 0s - loss: 2.9315e-0 - ETA: 0s - loss: 2.8651e-0 - ETA: 0s - loss: 2.8599e-0 - ETA: 0s - loss: 2.8376e-0 - ETA: 0s - loss: 2.8176e-0 - ETA: 0s - loss: 2.8116e-0 - ETA: 0s - loss: 2.8395e-0 - ETA: 0s - loss: 2.8292e-0 - ETA: 0s - loss: 2.8229e-0 - ETA: 0s - loss: 2.7908e-0 - ETA: 0s - loss: 2.7634e-0 - ETA: 0s - loss: 2.7258e-0 - ETA: 0s - loss: 2.7306e-0 - ETA: 0s - loss: 2.7173e-0 - 1s 565us/step - loss: 2.7460e-04 - val_loss: 0.0030\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2568/2568 [==============================] - ETA: 1s - loss: 4.7446e-0 - ETA: 1s - loss: 3.2490e-0 - ETA: 1s - loss: 2.8210e-0 - ETA: 1s - loss: 2.6565e-0 - ETA: 1s - loss: 2.5460e-0 - ETA: 1s - loss: 2.6633e-0 - ETA: 1s - loss: 2.7122e-0 - ETA: 0s - loss: 2.5846e-0 - ETA: 0s - loss: 2.7419e-0 - ETA: 0s - loss: 2.6656e-0 - ETA: 0s - loss: 2.6580e-0 - ETA: 0s - loss: 2.8411e-0 - ETA: 0s - loss: 2.9694e-0 - ETA: 0s - loss: 3.0274e-0 - ETA: 0s - loss: 2.9263e-0 - ETA: 0s - loss: 2.8741e-0 - ETA: 0s - loss: 2.8326e-0 - ETA: 0s - loss: 2.7972e-0 - ETA: 0s - loss: 2.8343e-0 - ETA: 0s - loss: 2.8937e-0 - ETA: 0s - loss: 2.8794e-0 - ETA: 0s - loss: 2.8769e-0 - ETA: 0s - loss: 2.8448e-0 - ETA: 0s - loss: 2.8004e-0 - 1s 568us/step - loss: 2.7644e-04 - val_loss: 7.5153e-04\n",
      "Epoch 45/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 2.1551e-0 - ETA: 1s - loss: 3.0061e-0 - ETA: 1s - loss: 2.8878e-0 - ETA: 1s - loss: 2.7133e-0 - ETA: 1s - loss: 2.7771e-0 - ETA: 1s - loss: 2.9357e-0 - ETA: 1s - loss: 2.8210e-0 - ETA: 0s - loss: 2.8097e-0 - ETA: 0s - loss: 2.7604e-0 - ETA: 0s - loss: 2.8195e-0 - ETA: 0s - loss: 2.8260e-0 - ETA: 0s - loss: 2.8232e-0 - ETA: 0s - loss: 2.8712e-0 - ETA: 0s - loss: 2.9712e-0 - ETA: 0s - loss: 2.9383e-0 - ETA: 0s - loss: 2.8403e-0 - ETA: 0s - loss: 2.7871e-0 - ETA: 0s - loss: 2.7786e-0 - ETA: 0s - loss: 2.8005e-0 - ETA: 0s - loss: 2.8144e-0 - ETA: 0s - loss: 2.8155e-0 - ETA: 0s - loss: 2.7891e-0 - ETA: 0s - loss: 2.7826e-0 - ETA: 0s - loss: 2.7590e-0 - 1s 555us/step - loss: 2.7598e-04 - val_loss: 7.9330e-04\n",
      "Epoch 46/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 3.1788e-0 - ETA: 1s - loss: 2.7592e-0 - ETA: 1s - loss: 2.4818e-0 - ETA: 1s - loss: 2.3573e-0 - ETA: 0s - loss: 2.6131e-0 - ETA: 0s - loss: 2.9415e-0 - ETA: 0s - loss: 2.9730e-0 - ETA: 0s - loss: 2.9314e-0 - ETA: 0s - loss: 2.9683e-0 - ETA: 0s - loss: 2.9417e-0 - ETA: 0s - loss: 2.8413e-0 - ETA: 0s - loss: 2.7632e-0 - ETA: 0s - loss: 2.7634e-0 - ETA: 0s - loss: 2.8503e-0 - ETA: 0s - loss: 2.8459e-0 - ETA: 0s - loss: 2.8514e-0 - ETA: 0s - loss: 2.7905e-0 - ETA: 0s - loss: 2.7509e-0 - ETA: 0s - loss: 2.6748e-0 - ETA: 0s - loss: 2.6396e-0 - ETA: 0s - loss: 2.6664e-0 - 1s 517us/step - loss: 2.6728e-04 - val_loss: 7.1663e-04\n",
      "Epoch 47/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 3.5971e-0 - ETA: 1s - loss: 2.9010e-0 - ETA: 1s - loss: 2.8626e-0 - ETA: 1s - loss: 2.6890e-0 - ETA: 0s - loss: 2.5063e-0 - ETA: 0s - loss: 2.7212e-0 - ETA: 0s - loss: 2.5201e-0 - ETA: 0s - loss: 2.5546e-0 - ETA: 0s - loss: 2.7000e-0 - ETA: 0s - loss: 2.7121e-0 - ETA: 0s - loss: 2.7221e-0 - ETA: 0s - loss: 2.8192e-0 - ETA: 0s - loss: 2.8102e-0 - ETA: 0s - loss: 2.7648e-0 - ETA: 0s - loss: 2.7754e-0 - ETA: 0s - loss: 2.7646e-0 - ETA: 0s - loss: 2.7297e-0 - ETA: 0s - loss: 2.8075e-0 - ETA: 0s - loss: 2.7541e-0 - ETA: 0s - loss: 2.7185e-0 - ETA: 0s - loss: 2.7208e-0 - 1s 532us/step - loss: 2.7056e-04 - val_loss: 9.1554e-04\n",
      "Epoch 48/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 1.2709e-0 - ETA: 1s - loss: 2.8331e-0 - ETA: 1s - loss: 2.8202e-0 - ETA: 1s - loss: 2.6571e-0 - ETA: 1s - loss: 2.6880e-0 - ETA: 1s - loss: 2.6034e-0 - ETA: 0s - loss: 2.8755e-0 - ETA: 0s - loss: 2.7263e-0 - ETA: 0s - loss: 2.7060e-0 - ETA: 0s - loss: 2.6569e-0 - ETA: 0s - loss: 2.7403e-0 - ETA: 0s - loss: 2.7901e-0 - ETA: 0s - loss: 2.7358e-0 - ETA: 0s - loss: 2.7462e-0 - ETA: 0s - loss: 2.7491e-0 - ETA: 0s - loss: 2.6945e-0 - ETA: 0s - loss: 2.6869e-0 - ETA: 0s - loss: 2.6503e-0 - ETA: 0s - loss: 2.6462e-0 - ETA: 0s - loss: 2.6338e-0 - ETA: 0s - loss: 2.6385e-0 - ETA: 0s - loss: 2.6241e-0 - 1s 532us/step - loss: 2.6496e-04 - val_loss: 7.0615e-04\n",
      "Epoch 49/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 3.8150e-0 - ETA: 1s - loss: 3.3635e-0 - ETA: 1s - loss: 3.3428e-0 - ETA: 1s - loss: 3.0652e-0 - ETA: 1s - loss: 2.7243e-0 - ETA: 0s - loss: 2.6616e-0 - ETA: 0s - loss: 2.5650e-0 - ETA: 0s - loss: 2.5857e-0 - ETA: 0s - loss: 2.4886e-0 - ETA: 0s - loss: 2.4536e-0 - ETA: 0s - loss: 2.5180e-0 - ETA: 0s - loss: 2.5105e-0 - ETA: 0s - loss: 2.6012e-0 - ETA: 0s - loss: 2.5862e-0 - ETA: 0s - loss: 2.5860e-0 - ETA: 0s - loss: 2.5634e-0 - ETA: 0s - loss: 2.5826e-0 - ETA: 0s - loss: 2.6154e-0 - ETA: 0s - loss: 2.5835e-0 - ETA: 0s - loss: 2.5758e-0 - ETA: 0s - loss: 2.5601e-0 - 1s 515us/step - loss: 2.6014e-04 - val_loss: 0.0015\n",
      "Epoch 50/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 1.8379e-0 - ETA: 1s - loss: 1.8526e-0 - ETA: 1s - loss: 2.0453e-0 - ETA: 1s - loss: 2.1091e-0 - ETA: 0s - loss: 2.0482e-0 - ETA: 0s - loss: 2.3286e-0 - ETA: 0s - loss: 2.5412e-0 - ETA: 0s - loss: 2.5876e-0 - ETA: 0s - loss: 2.5822e-0 - ETA: 0s - loss: 2.5783e-0 - ETA: 0s - loss: 2.6394e-0 - ETA: 0s - loss: 2.5854e-0 - ETA: 0s - loss: 2.5189e-0 - ETA: 0s - loss: 2.5168e-0 - ETA: 0s - loss: 2.5350e-0 - ETA: 0s - loss: 2.6166e-0 - ETA: 0s - loss: 2.6285e-0 - ETA: 0s - loss: 2.5798e-0 - ETA: 0s - loss: 2.5643e-0 - ETA: 0s - loss: 2.5599e-0 - ETA: 0s - loss: 2.5535e-0 - ETA: 0s - loss: 2.5485e-0 - ETA: 0s - loss: 2.5808e-0 - ETA: 0s - loss: 2.5855e-0 - 1s 550us/step - loss: 2.5801e-04 - val_loss: 9.4861e-04\n",
      "Epoch 51/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 3.5054e-0 - ETA: 1s - loss: 2.7132e-0 - ETA: 1s - loss: 2.6739e-0 - ETA: 1s - loss: 2.8624e-0 - ETA: 1s - loss: 2.7014e-0 - ETA: 1s - loss: 2.6623e-0 - ETA: 0s - loss: 2.5677e-0 - ETA: 0s - loss: 2.6054e-0 - ETA: 0s - loss: 2.5531e-0 - ETA: 0s - loss: 2.4846e-0 - ETA: 0s - loss: 2.4368e-0 - ETA: 0s - loss: 2.3680e-0 - ETA: 0s - loss: 2.3980e-0 - ETA: 0s - loss: 2.4863e-0 - ETA: 0s - loss: 2.4655e-0 - ETA: 0s - loss: 2.4828e-0 - ETA: 0s - loss: 2.5308e-0 - ETA: 0s - loss: 2.4939e-0 - ETA: 0s - loss: 2.5015e-0 - ETA: 0s - loss: 2.5253e-0 - ETA: 0s - loss: 2.5294e-0 - ETA: 0s - loss: 2.5450e-0 - ETA: 0s - loss: 2.5397e-0 - ETA: 0s - loss: 2.5445e-0 - 1s 559us/step - loss: 2.5410e-04 - val_loss: 0.0012\n",
      "Epoch 52/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 2.1673e-0 - ETA: 1s - loss: 2.4628e-0 - ETA: 1s - loss: 2.4721e-0 - ETA: 1s - loss: 2.5674e-0 - ETA: 1s - loss: 2.5702e-0 - ETA: 1s - loss: 2.8137e-0 - ETA: 0s - loss: 2.7343e-0 - ETA: 0s - loss: 2.6766e-0 - ETA: 0s - loss: 2.7811e-0 - ETA: 0s - loss: 2.7016e-0 - ETA: 0s - loss: 2.6631e-0 - ETA: 0s - loss: 2.7117e-0 - ETA: 0s - loss: 2.8494e-0 - ETA: 0s - loss: 2.8162e-0 - ETA: 0s - loss: 2.7296e-0 - ETA: 0s - loss: 2.7995e-0 - ETA: 0s - loss: 2.8051e-0 - ETA: 0s - loss: 2.7854e-0 - ETA: 0s - loss: 2.7686e-0 - ETA: 0s - loss: 2.7515e-0 - ETA: 0s - loss: 2.7149e-0 - ETA: 0s - loss: 2.6672e-0 - ETA: 0s - loss: 2.6552e-0 - ETA: 0s - loss: 2.6350e-0 - ETA: 0s - loss: 2.5963e-0 - ETA: 0s - loss: 2.5654e-0 - 2s 590us/step - loss: 2.5665e-04 - val_loss: 8.3129e-04\n",
      "Epoch 53/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 3.8911e-0 - ETA: 1s - loss: 2.6347e-0 - ETA: 1s - loss: 2.5442e-0 - ETA: 1s - loss: 2.5364e-0 - ETA: 1s - loss: 2.6526e-0 - ETA: 0s - loss: 2.6340e-0 - ETA: 0s - loss: 2.6131e-0 - ETA: 0s - loss: 2.5655e-0 - ETA: 0s - loss: 2.5547e-0 - ETA: 0s - loss: 2.6455e-0 - ETA: 0s - loss: 2.6586e-0 - ETA: 0s - loss: 2.6172e-0 - ETA: 0s - loss: 2.6482e-0 - ETA: 0s - loss: 2.6282e-0 - ETA: 0s - loss: 2.6171e-0 - ETA: 0s - loss: 2.6442e-0 - ETA: 0s - loss: 2.5960e-0 - ETA: 0s - loss: 2.5892e-0 - ETA: 0s - loss: 2.5652e-0 - ETA: 0s - loss: 2.5577e-0 - ETA: 0s - loss: 2.5548e-0 - ETA: 0s - loss: 2.5433e-0 - 1s 522us/step - loss: 2.5076e-04 - val_loss: 0.0021\n",
      "Epoch 54/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 2.9357e-0 - ETA: 0s - loss: 2.7193e-0 - ETA: 0s - loss: 2.6048e-0 - ETA: 0s - loss: 2.5004e-0 - ETA: 0s - loss: 2.4425e-0 - ETA: 0s - loss: 2.4002e-0 - ETA: 0s - loss: 2.4804e-0 - ETA: 0s - loss: 2.4885e-0 - ETA: 0s - loss: 2.4379e-0 - ETA: 0s - loss: 2.4236e-0 - ETA: 0s - loss: 2.4220e-0 - ETA: 0s - loss: 2.4458e-0 - ETA: 0s - loss: 2.4179e-0 - ETA: 0s - loss: 2.3832e-0 - ETA: 0s - loss: 2.3775e-0 - ETA: 0s - loss: 2.3685e-0 - ETA: 0s - loss: 2.4148e-0 - ETA: 0s - loss: 2.4741e-0 - ETA: 0s - loss: 2.4555e-0 - ETA: 0s - loss: 2.5343e-0 - 1s 451us/step - loss: 2.5187e-04 - val_loss: 0.0011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 2.3647e-0 - ETA: 1s - loss: 2.6160e-0 - ETA: 1s - loss: 2.9263e-0 - ETA: 1s - loss: 2.4962e-0 - ETA: 1s - loss: 2.4132e-0 - ETA: 1s - loss: 2.3646e-0 - ETA: 0s - loss: 2.5423e-0 - ETA: 0s - loss: 2.4920e-0 - ETA: 0s - loss: 2.4836e-0 - ETA: 0s - loss: 2.4265e-0 - ETA: 0s - loss: 2.3737e-0 - ETA: 0s - loss: 2.4303e-0 - ETA: 0s - loss: 2.3999e-0 - ETA: 0s - loss: 2.3591e-0 - ETA: 0s - loss: 2.3412e-0 - ETA: 0s - loss: 2.3982e-0 - ETA: 0s - loss: 2.3936e-0 - ETA: 0s - loss: 2.3651e-0 - ETA: 0s - loss: 2.4132e-0 - ETA: 0s - loss: 2.3986e-0 - ETA: 0s - loss: 2.3843e-0 - ETA: 0s - loss: 2.3542e-0 - ETA: 0s - loss: 2.3831e-0 - ETA: 0s - loss: 2.4098e-0 - 1s 582us/step - loss: 2.4654e-04 - val_loss: 7.3897e-04\n",
      "Epoch 56/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 4.6596e-0 - ETA: 1s - loss: 2.9128e-0 - ETA: 1s - loss: 2.6252e-0 - ETA: 1s - loss: 2.4829e-0 - ETA: 1s - loss: 2.3780e-0 - ETA: 1s - loss: 2.4679e-0 - ETA: 0s - loss: 2.2945e-0 - ETA: 0s - loss: 2.4090e-0 - ETA: 0s - loss: 2.5469e-0 - ETA: 0s - loss: 2.5248e-0 - ETA: 0s - loss: 2.4588e-0 - ETA: 0s - loss: 2.3705e-0 - ETA: 0s - loss: 2.3270e-0 - ETA: 0s - loss: 2.4234e-0 - ETA: 0s - loss: 2.5336e-0 - ETA: 0s - loss: 2.5424e-0 - ETA: 0s - loss: 2.5262e-0 - ETA: 0s - loss: 2.5173e-0 - ETA: 0s - loss: 2.5142e-0 - ETA: 0s - loss: 2.4841e-0 - ETA: 0s - loss: 2.4508e-0 - ETA: 0s - loss: 2.4780e-0 - 1s 546us/step - loss: 2.4718e-04 - val_loss: 0.0013\n",
      "Epoch 57/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 1.2875e-0 - ETA: 1s - loss: 2.0722e-0 - ETA: 1s - loss: 2.1516e-0 - ETA: 1s - loss: 2.1601e-0 - ETA: 0s - loss: 2.3765e-0 - ETA: 0s - loss: 2.2795e-0 - ETA: 0s - loss: 2.4153e-0 - ETA: 0s - loss: 2.6917e-0 - ETA: 0s - loss: 2.5768e-0 - ETA: 0s - loss: 2.6002e-0 - ETA: 0s - loss: 2.5553e-0 - ETA: 0s - loss: 2.5164e-0 - ETA: 0s - loss: 2.4858e-0 - ETA: 0s - loss: 2.4940e-0 - ETA: 0s - loss: 2.4202e-0 - ETA: 0s - loss: 2.4177e-0 - ETA: 0s - loss: 2.3889e-0 - ETA: 0s - loss: 2.3889e-0 - ETA: 0s - loss: 2.3785e-0 - ETA: 0s - loss: 2.3992e-0 - ETA: 0s - loss: 2.3975e-0 - 1s 522us/step - loss: 2.4305e-04 - val_loss: 0.0023\n",
      "Epoch 58/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 2.3470e-0 - ETA: 1s - loss: 3.5786e-0 - ETA: 1s - loss: 2.7566e-0 - ETA: 1s - loss: 2.6860e-0 - ETA: 1s - loss: 2.3923e-0 - ETA: 1s - loss: 2.4016e-0 - ETA: 1s - loss: 2.3951e-0 - ETA: 1s - loss: 2.3521e-0 - ETA: 1s - loss: 2.4442e-0 - ETA: 0s - loss: 2.4220e-0 - ETA: 0s - loss: 2.4110e-0 - ETA: 0s - loss: 2.4537e-0 - ETA: 0s - loss: 2.3871e-0 - ETA: 0s - loss: 2.3517e-0 - ETA: 0s - loss: 2.3206e-0 - ETA: 0s - loss: 2.3078e-0 - ETA: 0s - loss: 2.3878e-0 - ETA: 0s - loss: 2.3885e-0 - ETA: 0s - loss: 2.3434e-0 - ETA: 0s - loss: 2.4047e-0 - ETA: 0s - loss: 2.3924e-0 - ETA: 0s - loss: 2.3680e-0 - ETA: 0s - loss: 2.3691e-0 - ETA: 0s - loss: 2.3582e-0 - 1s 566us/step - loss: 2.3667e-04 - val_loss: 0.0023\n",
      "Epoch 59/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 2.0208e-0 - ETA: 1s - loss: 2.6103e-0 - ETA: 1s - loss: 2.3612e-0 - ETA: 1s - loss: 2.3035e-0 - ETA: 1s - loss: 2.3326e-0 - ETA: 1s - loss: 2.3736e-0 - ETA: 0s - loss: 2.5802e-0 - ETA: 0s - loss: 2.6175e-0 - ETA: 0s - loss: 2.6738e-0 - ETA: 0s - loss: 2.6444e-0 - ETA: 0s - loss: 2.6333e-0 - ETA: 0s - loss: 2.6192e-0 - ETA: 0s - loss: 2.5226e-0 - ETA: 0s - loss: 2.4795e-0 - ETA: 0s - loss: 2.4250e-0 - ETA: 0s - loss: 2.5239e-0 - ETA: 0s - loss: 2.5636e-0 - ETA: 0s - loss: 2.5127e-0 - ETA: 0s - loss: 2.4892e-0 - ETA: 0s - loss: 2.4728e-0 - ETA: 0s - loss: 2.5039e-0 - ETA: 0s - loss: 2.4789e-0 - ETA: 0s - loss: 2.4652e-0 - ETA: 0s - loss: 2.4113e-0 - ETA: 0s - loss: 2.4038e-0 - 2s 586us/step - loss: 2.4093e-04 - val_loss: 0.0021\n",
      "Epoch 60/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 1.0592e-0 - ETA: 1s - loss: 2.2642e-0 - ETA: 1s - loss: 2.1738e-0 - ETA: 1s - loss: 2.1712e-0 - ETA: 1s - loss: 2.3873e-0 - ETA: 0s - loss: 2.2777e-0 - ETA: 0s - loss: 2.4728e-0 - ETA: 0s - loss: 2.4298e-0 - ETA: 0s - loss: 2.3687e-0 - ETA: 0s - loss: 2.3591e-0 - ETA: 0s - loss: 2.3515e-0 - ETA: 0s - loss: 2.2853e-0 - ETA: 0s - loss: 2.3123e-0 - ETA: 0s - loss: 2.3252e-0 - ETA: 0s - loss: 2.3336e-0 - ETA: 0s - loss: 2.3291e-0 - ETA: 0s - loss: 2.3618e-0 - ETA: 0s - loss: 2.4203e-0 - ETA: 0s - loss: 2.4322e-0 - ETA: 0s - loss: 2.3972e-0 - ETA: 0s - loss: 2.3945e-0 - 1s 514us/step - loss: 2.3794e-04 - val_loss: 0.0024\n",
      "Epoch 61/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 2.7739e-0 - ETA: 1s - loss: 2.4131e-0 - ETA: 0s - loss: 2.1327e-0 - ETA: 0s - loss: 2.0946e-0 - ETA: 0s - loss: 2.1957e-0 - ETA: 0s - loss: 2.1831e-0 - ETA: 0s - loss: 2.2580e-0 - ETA: 0s - loss: 2.3680e-0 - ETA: 0s - loss: 2.4619e-0 - ETA: 0s - loss: 2.5492e-0 - ETA: 0s - loss: 2.6444e-0 - ETA: 0s - loss: 2.5875e-0 - ETA: 0s - loss: 2.5039e-0 - ETA: 0s - loss: 2.4571e-0 - ETA: 0s - loss: 2.4289e-0 - ETA: 0s - loss: 2.4164e-0 - ETA: 0s - loss: 2.4041e-0 - ETA: 0s - loss: 2.3715e-0 - ETA: 0s - loss: 2.3745e-0 - ETA: 0s - loss: 2.3968e-0 - ETA: 0s - loss: 2.3812e-0 - 1s 482us/step - loss: 2.3782e-04 - val_loss: 0.0021\n",
      "Epoch 62/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 2.1455e-0 - ETA: 1s - loss: 2.0175e-0 - ETA: 0s - loss: 1.9471e-0 - ETA: 0s - loss: 2.1165e-0 - ETA: 0s - loss: 2.0171e-0 - ETA: 0s - loss: 2.1030e-0 - ETA: 0s - loss: 2.1240e-0 - ETA: 0s - loss: 2.1564e-0 - ETA: 0s - loss: 2.1171e-0 - ETA: 0s - loss: 2.1863e-0 - ETA: 0s - loss: 2.2164e-0 - ETA: 0s - loss: 2.2783e-0 - ETA: 0s - loss: 2.2550e-0 - ETA: 0s - loss: 2.2564e-0 - ETA: 0s - loss: 2.2859e-0 - ETA: 0s - loss: 2.2537e-0 - ETA: 0s - loss: 2.2888e-0 - ETA: 0s - loss: 2.3207e-0 - ETA: 0s - loss: 2.3034e-0 - ETA: 0s - loss: 2.3242e-0 - 1s 477us/step - loss: 2.3412e-04 - val_loss: 9.3353e-04\n",
      "Epoch 63/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 4.1056e-0 - ETA: 0s - loss: 2.8424e-0 - ETA: 0s - loss: 2.6152e-0 - ETA: 0s - loss: 2.3546e-0 - ETA: 0s - loss: 2.2370e-0 - ETA: 0s - loss: 2.2219e-0 - ETA: 0s - loss: 2.2009e-0 - ETA: 0s - loss: 2.1814e-0 - ETA: 0s - loss: 2.3903e-0 - ETA: 0s - loss: 2.3641e-0 - ETA: 0s - loss: 2.4279e-0 - ETA: 0s - loss: 2.3696e-0 - ETA: 0s - loss: 2.4197e-0 - ETA: 0s - loss: 2.3983e-0 - ETA: 0s - loss: 2.3744e-0 - ETA: 0s - loss: 2.4548e-0 - ETA: 0s - loss: 2.4303e-0 - ETA: 0s - loss: 2.3959e-0 - ETA: 0s - loss: 2.3945e-0 - ETA: 0s - loss: 2.3694e-0 - 1s 455us/step - loss: 2.3616e-04 - val_loss: 0.0018\n",
      "Epoch 64/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 1.2855e-0 - ETA: 0s - loss: 2.5715e-0 - ETA: 0s - loss: 2.7115e-0 - ETA: 0s - loss: 2.6967e-0 - ETA: 0s - loss: 2.5230e-0 - ETA: 0s - loss: 2.7043e-0 - ETA: 0s - loss: 2.5881e-0 - ETA: 0s - loss: 2.4823e-0 - ETA: 0s - loss: 2.3830e-0 - ETA: 0s - loss: 2.3962e-0 - ETA: 0s - loss: 2.3431e-0 - ETA: 0s - loss: 2.3054e-0 - ETA: 0s - loss: 2.2502e-0 - ETA: 0s - loss: 2.2307e-0 - ETA: 0s - loss: 2.2313e-0 - ETA: 0s - loss: 2.2053e-0 - ETA: 0s - loss: 2.1964e-0 - ETA: 0s - loss: 2.2722e-0 - ETA: 0s - loss: 2.2627e-0 - 1s 449us/step - loss: 2.2864e-04 - val_loss: 0.0046\n",
      "Epoch 65/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 3.3554e-0 - ETA: 1s - loss: 3.6294e-0 - ETA: 0s - loss: 3.1345e-0 - ETA: 0s - loss: 2.9798e-0 - ETA: 0s - loss: 2.7527e-0 - ETA: 0s - loss: 2.5810e-0 - ETA: 0s - loss: 2.5332e-0 - ETA: 0s - loss: 2.4201e-0 - ETA: 0s - loss: 2.3485e-0 - ETA: 0s - loss: 2.3912e-0 - ETA: 0s - loss: 2.3275e-0 - ETA: 0s - loss: 2.4007e-0 - ETA: 0s - loss: 2.4578e-0 - ETA: 0s - loss: 2.4120e-0 - ETA: 0s - loss: 2.3339e-0 - ETA: 0s - loss: 2.3666e-0 - ETA: 0s - loss: 2.3801e-0 - ETA: 0s - loss: 2.3493e-0 - ETA: 0s - loss: 2.3443e-0 - ETA: 0s - loss: 2.3533e-0 - 1s 453us/step - loss: 2.3202e-04 - val_loss: 0.0017\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2568/2568 [==============================] - ETA: 0s - loss: 2.9949e-0 - ETA: 0s - loss: 2.5164e-0 - ETA: 0s - loss: 2.4434e-0 - ETA: 0s - loss: 2.4714e-0 - ETA: 0s - loss: 2.4468e-0 - ETA: 0s - loss: 2.3982e-0 - ETA: 0s - loss: 2.5028e-0 - ETA: 0s - loss: 2.4323e-0 - ETA: 0s - loss: 2.4536e-0 - ETA: 0s - loss: 2.4146e-0 - ETA: 0s - loss: 2.3576e-0 - ETA: 0s - loss: 2.4257e-0 - ETA: 0s - loss: 2.4074e-0 - ETA: 0s - loss: 2.3550e-0 - ETA: 0s - loss: 2.3183e-0 - ETA: 0s - loss: 2.3525e-0 - ETA: 0s - loss: 2.3526e-0 - ETA: 0s - loss: 2.3430e-0 - ETA: 0s - loss: 2.3398e-0 - ETA: 0s - loss: 2.3361e-0 - 1s 473us/step - loss: 2.3334e-04 - val_loss: 0.0031\n",
      "Epoch 67/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 1.6474e-0 - ETA: 1s - loss: 2.2994e-0 - ETA: 1s - loss: 2.3259e-0 - ETA: 1s - loss: 2.4712e-0 - ETA: 0s - loss: 2.8400e-0 - ETA: 0s - loss: 2.6521e-0 - ETA: 0s - loss: 2.5495e-0 - ETA: 0s - loss: 2.4920e-0 - ETA: 0s - loss: 2.4584e-0 - ETA: 0s - loss: 2.5028e-0 - ETA: 0s - loss: 2.4366e-0 - ETA: 0s - loss: 2.4455e-0 - ETA: 0s - loss: 2.4347e-0 - ETA: 0s - loss: 2.4129e-0 - ETA: 0s - loss: 2.3925e-0 - ETA: 0s - loss: 2.3784e-0 - ETA: 0s - loss: 2.3565e-0 - ETA: 0s - loss: 2.3351e-0 - ETA: 0s - loss: 2.3175e-0 - ETA: 0s - loss: 2.3093e-0 - ETA: 0s - loss: 2.3582e-0 - 1s 521us/step - loss: 2.3604e-04 - val_loss: 0.0019\n",
      "Epoch 68/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 2.6161e-0 - ETA: 1s - loss: 1.8781e-0 - ETA: 1s - loss: 1.8723e-0 - ETA: 1s - loss: 1.8274e-0 - ETA: 0s - loss: 1.9648e-0 - ETA: 0s - loss: 2.2855e-0 - ETA: 0s - loss: 2.3002e-0 - ETA: 0s - loss: 2.2726e-0 - ETA: 0s - loss: 2.3232e-0 - ETA: 0s - loss: 2.3135e-0 - ETA: 0s - loss: 2.2831e-0 - ETA: 0s - loss: 2.3261e-0 - ETA: 0s - loss: 2.2600e-0 - ETA: 0s - loss: 2.2344e-0 - ETA: 0s - loss: 2.2888e-0 - ETA: 0s - loss: 2.2435e-0 - ETA: 0s - loss: 2.1990e-0 - ETA: 0s - loss: 2.1786e-0 - ETA: 0s - loss: 2.2394e-0 - ETA: 0s - loss: 2.2626e-0 - ETA: 0s - loss: 2.2373e-0 - 1s 519us/step - loss: 2.2457e-04 - val_loss: 0.0010\n",
      "Epoch 69/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 4.1471e-0 - ETA: 1s - loss: 2.0924e-0 - ETA: 1s - loss: 2.2569e-0 - ETA: 0s - loss: 2.1838e-0 - ETA: 0s - loss: 2.2743e-0 - ETA: 0s - loss: 2.1171e-0 - ETA: 0s - loss: 2.0851e-0 - ETA: 0s - loss: 2.0198e-0 - ETA: 0s - loss: 2.0175e-0 - ETA: 0s - loss: 2.0371e-0 - ETA: 0s - loss: 2.0485e-0 - ETA: 0s - loss: 2.0409e-0 - ETA: 0s - loss: 2.0525e-0 - ETA: 0s - loss: 2.0936e-0 - ETA: 0s - loss: 2.1503e-0 - ETA: 0s - loss: 2.1340e-0 - ETA: 0s - loss: 2.1862e-0 - ETA: 0s - loss: 2.1561e-0 - ETA: 0s - loss: 2.1881e-0 - ETA: 0s - loss: 2.1965e-0 - ETA: 0s - loss: 2.2022e-0 - ETA: 0s - loss: 2.2315e-0 - 1s 512us/step - loss: 2.2337e-04 - val_loss: 0.0013\n",
      "Epoch 70/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 1.5720e-0 - ETA: 0s - loss: 2.4850e-0 - ETA: 1s - loss: 2.2529e-0 - ETA: 0s - loss: 2.2471e-0 - ETA: 0s - loss: 2.1748e-0 - ETA: 0s - loss: 2.0458e-0 - ETA: 0s - loss: 2.0827e-0 - ETA: 0s - loss: 2.1213e-0 - ETA: 0s - loss: 2.1046e-0 - ETA: 0s - loss: 2.1271e-0 - ETA: 0s - loss: 2.1324e-0 - ETA: 0s - loss: 2.0995e-0 - ETA: 0s - loss: 2.0885e-0 - ETA: 0s - loss: 2.0772e-0 - ETA: 0s - loss: 2.1271e-0 - ETA: 0s - loss: 2.2173e-0 - ETA: 0s - loss: 2.2548e-0 - ETA: 0s - loss: 2.2319e-0 - ETA: 0s - loss: 2.2709e-0 - ETA: 0s - loss: 2.2506e-0 - 1s 501us/step - loss: 2.2489e-04 - val_loss: 0.0017\n",
      "Epoch 71/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 1.5201e-0 - ETA: 0s - loss: 2.2684e-0 - ETA: 0s - loss: 2.4005e-0 - ETA: 0s - loss: 2.4835e-0 - ETA: 0s - loss: 2.4800e-0 - ETA: 0s - loss: 2.5747e-0 - ETA: 0s - loss: 2.4772e-0 - ETA: 0s - loss: 2.3854e-0 - ETA: 0s - loss: 2.5016e-0 - ETA: 0s - loss: 2.4283e-0 - ETA: 0s - loss: 2.3592e-0 - ETA: 0s - loss: 2.3571e-0 - ETA: 0s - loss: 2.3328e-0 - ETA: 0s - loss: 2.2977e-0 - ETA: 0s - loss: 2.2798e-0 - ETA: 0s - loss: 2.2568e-0 - ETA: 0s - loss: 2.2847e-0 - ETA: 0s - loss: 2.2591e-0 - ETA: 0s - loss: 2.2456e-0 - ETA: 0s - loss: 2.2745e-0 - ETA: 0s - loss: 2.2599e-0 - 1s 503us/step - loss: 2.2594e-04 - val_loss: 0.0039\n",
      "Epoch 72/100\n",
      "2568/2568 [==============================] - ETA: 0s - loss: 3.7537e-0 - ETA: 1s - loss: 3.0659e-0 - ETA: 1s - loss: 2.6047e-0 - ETA: 0s - loss: 2.9500e-0 - ETA: 0s - loss: 2.7017e-0 - ETA: 0s - loss: 2.5772e-0 - ETA: 0s - loss: 2.4680e-0 - ETA: 0s - loss: 2.4282e-0 - ETA: 0s - loss: 2.3762e-0 - ETA: 0s - loss: 2.4125e-0 - ETA: 0s - loss: 2.3986e-0 - ETA: 0s - loss: 2.4052e-0 - ETA: 0s - loss: 2.3648e-0 - ETA: 0s - loss: 2.3094e-0 - ETA: 0s - loss: 2.3108e-0 - ETA: 0s - loss: 2.2883e-0 - ETA: 0s - loss: 2.2953e-0 - ETA: 0s - loss: 2.3017e-0 - ETA: 0s - loss: 2.2816e-0 - ETA: 0s - loss: 2.2433e-0 - 1s 500us/step - loss: 2.2276e-04 - val_loss: 0.0012\n",
      "Epoch 73/100\n",
      "2568/2568 [==============================] - ETA: 1s - loss: 2.7327e-0 - ETA: 0s - loss: 1.8468e-0 - ETA: 0s - loss: 1.7364e-0 - ETA: 0s - loss: 2.1688e-0 - ETA: 0s - loss: 2.1634e-0 - ETA: 0s - loss: 2.0739e-0 - ETA: 0s - loss: 2.0393e-0 - ETA: 0s - loss: 2.0570e-0 - ETA: 0s - loss: 2.0311e-0 - ETA: 0s - loss: 2.0762e-0 - ETA: 0s - loss: 2.1596e-0 - ETA: 0s - loss: 2.1326e-0 - ETA: 0s - loss: 2.1288e-0 - ETA: 0s - loss: 2.0886e-0 - ETA: 0s - loss: 2.1285e-0 - ETA: 0s - loss: 2.1635e-0 - ETA: 0s - loss: 2.1380e-0 - ETA: 0s - loss: 2.1724e-0 - ETA: 0s - loss: 2.2380e-0 - ETA: 0s - loss: 2.2360e-0 - 1s 473us/step - loss: 2.2193e-04 - val_loss: 0.0027\n",
      "Epoch 74/100\n",
      "1760/2568 [===================>..........] - ETA: 0s - loss: 1.3119e-0 - ETA: 0s - loss: 1.9895e-0 - ETA: 0s - loss: 1.9627e-0 - ETA: 0s - loss: 2.1107e-0 - ETA: 0s - loss: 2.0473e-0 - ETA: 0s - loss: 1.9495e-0 - ETA: 0s - loss: 1.9935e-0 - ETA: 0s - loss: 2.1501e-0 - ETA: 0s - loss: 2.1501e-0 - ETA: 0s - loss: 2.0619e-0 - ETA: 0s - loss: 2.0830e-0 - ETA: 0s - loss: 2.0863e-0 - ETA: 0s - loss: 2.0439e-0 - ETA: 0s - loss: 2.0405e-04"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "lstm_model = keras.models.Sequential()\n",
    "lstm_model.add(keras.layers.Conv1D(32, kernel_size=3, input_shape=(TIME_STEPS, 6)))\n",
    "lstm_model.add(keras.layers.MaxPooling1D(4))\n",
    "lstm_model.add(keras.layers.LSTM(32, return_sequences=True))\n",
    "lstm_model.add(keras.layers.LSTM(32, return_sequences=False))\n",
    "lstm_model.add(tf.keras.layers.Dense(DAYS_TO_PREDICT))\n",
    "\n",
    "lstm_model.summary()\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.0001)\n",
    "lstm_model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "# Tensorboard is primarily used for visualization of the model here\n",
    "# tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs\".format(time()))\n",
    "\n",
    "lstm_model.fit(xTrain, yTrain, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE/ REMINDER FOR LATER: LSTM output is of dimensions/shape (X, ) where X is number of test cases, but to inverse transform using the scaler, you need an output shape of (X, 1). Although both represent a linear array, the first one (X, ) is considered a one-dimensional numpy array and the second one (X, 1) is considered a two-dimensional numpy array. Thus, using reshape(-1, 1) converts (X, ) to (X, 1). Also note that np.squeeze removes all dimensions where dimension = 1. So for example, (X, 1) becomes (X, ). In this case, the lstm prediction output is the same as yPred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes values from between 0 and 1 back to USD values for plotting\n",
    "prediction = outputScaler.inverse_transform(lstm_model.predict(xTest).reshape(-1, 1))\n",
    "yPred = np.squeeze(prediction)\n",
    "test = np.squeeze(yTest)\n",
    "actual = outputScaler.inverse_transform(test.reshape(-1, 1))\n",
    "actual = np.squeeze(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.plot(yPred, label='Predicted')\n",
    "ax.plot(actual, label='Actual')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Price in USD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the simulation using test data, assume closing price for a day equals opening price for next day\n",
    "\n",
    "balance = 0\n",
    "balanceValues = []\n",
    "\n",
    "for index, openingVal in enumerate(actual[ :-1]):\n",
    "    day = index\n",
    "    closingPredicted = yPred[index + 1]\n",
    "    closingActual = actual[index + 1]\n",
    "    \n",
    "    # If prediction is larger than the opening value, then BUY\n",
    "    if closingPredicted > openingVal:\n",
    "        balance += closingActual - openingVal\n",
    "        \n",
    "    # If prediction is less than the opening value, do NOTHING\n",
    "    else:\n",
    "        pass\n",
    "    balanceValues.append(balance)\n",
    "    sleep(.1)\n",
    "    print('Day:', day, 'Opening Price:', round(openingVal, 3), 'Predicted Closing Price:', round(closingPredicted, 3),\n",
    "         'Actual Closing Price:', round(closingActual, 3), 'Balance', round(balance, 3))\n",
    "\n",
    "print('Final Balance:', balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the overall trend of the balance\n",
    "\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.plot(balanceValues, label='Balance')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Price in USD')\n",
    "ax.set_title('Simulation Balance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final thoughts: This model seems to predict the up/down trend pretty well, but there were a few simplifying assumptions made along the way of this analysis which probably influenced the outcome. Also its important to keep in mind that the actual vs predicted graph is expected to similar to each other since the LSTM is only predicting one value at a time, and by picking something similar to the previous 60 values, it will always fall within range of the actual value. The important prediction here is if it can reliably predict if a stock is going up or down the next day, which is what the balance graph shows.\n",
    "\n",
    "Although deep learning seems to have some potential in modeling relatively stochastic trends, it is not likely the best way to go about investing. Regarding possible ways of improving this model, the addition of more input data could help. Using the information from similar tech stocks to predict Microsoft's trends would probably reduce loss. Additional information regarding news could also potentially included in the model through seniment analysis from different news sources, although this would require a much more sophisticated approach than the one used here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
